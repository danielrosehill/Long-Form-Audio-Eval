1
00:00:00,080 --> 00:00:06,240
Hello and welcome to a audio dataset consisting of one

2
00:00:06,240 --> 00:00:08,400
single episode of a nonexistent podcast.

3
00:00:08,800 --> 00:00:12,880
Or it I may append this to a podcast that

4
00:00:12,880 --> 00:00:18,814
I set up recently regarding my with my thoughts on

5
00:00:18,815 --> 00:00:20,815
speech tech and A.

6
00:00:20,815 --> 00:00:21,214
I.

7
00:00:21,214 --> 00:00:22,814
In particular, more A.

8
00:00:22,814 --> 00:00:23,054
I.

9
00:00:23,054 --> 00:00:23,935
And generative A.

10
00:00:23,935 --> 00:00:24,095
I.

11
00:00:24,095 --> 00:00:26,494
I would I would say.

12
00:00:26,814 --> 00:00:30,869
But in any event, the purpose of this voice recording

13
00:00:30,869 --> 00:00:35,590
is actually to create a lengthy voice sample for a

14
00:00:35,590 --> 00:00:38,950
quick evaluation, a back of the envelope evaluation, they might

15
00:00:38,950 --> 00:00:41,429
say, for different speech attacks models.

16
00:00:41,429 --> 00:00:43,945
I'm doing this because I thought I'd made a great

17
00:00:43,945 --> 00:00:47,784
breakthrough in my journey with speech tech and that was

18
00:00:47,784 --> 00:00:51,385
succeeding in the elusive task of fine tuning whisper.

19
00:00:51,704 --> 00:00:56,424
Whisper is, and I'm to just talk, I'm trying to

20
00:00:55,829 --> 00:00:56,789
mix up.

21
00:00:56,869 --> 00:01:00,390
I'm going to try a few different styles of speaking

22
00:01:00,390 --> 00:01:02,869
whisper something at some points as well.

23
00:01:03,350 --> 00:01:06,790
And I'll go back to speaking loud in in different

24
00:01:06,790 --> 00:01:09,030
parts are going to sound really like a crazy person

25
00:01:09,030 --> 00:01:12,424
because I'm also going to try to speak at different

26
00:01:12,984 --> 00:01:18,025
pitches and cadences in order to really try to push

27
00:01:18,344 --> 00:01:21,145
a speech to text model through its paces, which is

28
00:01:21,145 --> 00:01:24,609
trying to make sense of is this guy just rambling

29
00:01:24,609 --> 00:01:30,049
on incoherently in one long sentence or are these just

30
00:01:30,049 --> 00:01:36,450
actually a series of step standalone, standalone, standalone sentences?

31
00:01:36,450 --> 00:01:38,130
And how is it going to handle step alone?

32
00:01:38,130 --> 00:01:38,770
That's not a word.

33
00:01:39,704 --> 00:01:42,025
What happens when you use speech to text and you

34
00:01:42,025 --> 00:01:43,384
use a fake word?

35
00:01:43,384 --> 00:01:45,784
And then you're like, wait, that's not actually that word

36
00:01:45,784 --> 00:01:46,665
doesn't exist.

37
00:01:46,984 --> 00:01:48,584
How does AI handle that?

38
00:01:48,584 --> 00:01:53,750
And these and more are all the questions that I'm

39
00:01:53,750 --> 00:01:55,750
seeking to answer in this training data.

40
00:01:55,829 --> 00:01:58,549
Now, why was I trying to fine tune Whisper?

41
00:01:58,549 --> 00:01:59,750
And what is Whisper?

42
00:01:59,750 --> 00:02:02,710
As I said, I'm going to try to record this

43
00:02:02,710 --> 00:02:06,644
at a couple of different levels of technicality for folks

44
00:02:06,644 --> 00:02:11,764
who are in the normal world and not totally stuck

45
00:02:11,764 --> 00:02:13,764
down the rabbit hole of AI, which you have to

46
00:02:13,764 --> 00:02:17,685
say is a really wonderful rabbit hole to be done.

47
00:02:17,844 --> 00:02:20,919
It's a really interesting area and speech and voice tech

48
00:02:20,919 --> 00:02:24,359
is is the aspect of it that I find actually

49
00:02:24,359 --> 00:02:27,239
most I'm not sure I would say the most interesting

50
00:02:27,239 --> 00:02:30,759
because there's just so much that is fascinating in AI.

51
00:02:31,400 --> 00:02:34,134
But the most that I find the most personally transformative

52
00:02:34,134 --> 00:02:38,534
in terms of the impact that it's had on my

53
00:02:38,534 --> 00:02:41,254
daily work life and productivity and how I sort of

54
00:02:41,254 --> 00:02:41,895
work.

55
00:02:42,935 --> 00:02:47,500
I'm persevering hard with the task of trying to get

56
00:02:47,500 --> 00:02:50,939
a good solution working for Linux, which if anyone actually

57
00:02:50,939 --> 00:02:52,939
does listen to this, not just for the training data

58
00:02:52,939 --> 00:02:56,700
and for the actual content, is sparked.

59
00:02:56,700 --> 00:02:59,980
I had, besides the fine tune not working, well that

60
00:02:59,980 --> 00:03:01,385
was the failure.

61
00:03:02,504 --> 00:03:06,745
I used Claude code because one thinks these days that

62
00:03:06,745 --> 00:03:13,280
there is nothing short of solving, you know, the the

63
00:03:13,280 --> 00:03:17,599
reason of life or something that clause and agentic AI

64
00:03:17,599 --> 00:03:19,680
can't do, which is not really the case.

65
00:03:19,680 --> 00:03:23,199
It does seem that way sometimes, but it fails a

66
00:03:23,199 --> 00:03:23,759
lot as well.

67
00:03:23,759 --> 00:03:26,639
And this is one of those instances where last week

68
00:03:26,639 --> 00:03:30,824
I put together an hour of voice training data, basically

69
00:03:30,824 --> 00:03:33,465
speaking just random things for three minutes.

70
00:03:35,465 --> 00:03:38,104
It was actually kind of tedious because the texts were

71
00:03:38,104 --> 00:03:38,664
really weird.

72
00:03:38,664 --> 00:03:41,370
Some of them were, it was like it was AI

73
00:03:41,370 --> 00:03:42,250
generated.

74
00:03:42,569 --> 00:03:44,889
I tried before to read Sherlock Holmes for an hour

75
00:03:44,889 --> 00:03:47,689
and I just couldn't, I was so bored after ten

76
00:03:47,689 --> 00:03:50,569
minutes that I was like, okay, no, I'm just gonna

77
00:03:50,569 --> 00:03:51,930
have to find something else to read.

78
00:03:51,930 --> 00:03:58,284
So I used a created with AI Studio, VibeCoded, a

79
00:03:58,284 --> 00:04:03,164
synthetic text generator which actually I thought was probably a

80
00:04:03,164 --> 00:04:05,245
better way of doing it because it would give me

81
00:04:05,245 --> 00:04:09,069
more short samples with more varied content.

82
00:04:09,069 --> 00:04:11,710
So I was like, okay, give me a voice note

83
00:04:11,710 --> 00:04:14,909
like I'm recording an email, give me a short story

84
00:04:14,909 --> 00:04:18,189
to read, give me prose to read.

85
00:04:18,189 --> 00:04:20,634
So I came up with all these different things and

86
00:04:20,634 --> 00:04:22,714
they added a little timer to it so I could

87
00:04:22,714 --> 00:04:24,955
see how close I was to one hour.

88
00:04:25,915 --> 00:04:29,115
And I spent like an hour one afternoon or probably

89
00:04:29,115 --> 00:04:33,115
two hours by the time you do retakes and whatever

90
00:04:33,115 --> 00:04:36,169
because you want to it gave me a source of

91
00:04:36,169 --> 00:04:40,009
truth which I'm not sure if that's the scientific way

92
00:04:40,009 --> 00:04:44,169
to approach this topic of gathering training data but I

93
00:04:44,169 --> 00:04:45,449
thought made sense.

94
00:04:46,490 --> 00:04:49,464
I have a lot of audio data from recording voice

95
00:04:49,464 --> 00:04:53,544
notes which I've also kind of used, been experimenting with

96
00:04:53,544 --> 00:04:55,064
using for a different purpose.

97
00:04:55,384 --> 00:04:58,745
Slightly different annotating task types.

98
00:04:58,745 --> 00:05:03,250
It's more a text classification experiment or Well, it's more

99
00:05:03,250 --> 00:05:03,810
than that actually.

100
00:05:03,810 --> 00:05:05,009
I'm working on a voice app.

101
00:05:05,009 --> 00:05:09,329
So it's a prototype, I guess, is really more accurate.

102
00:05:11,409 --> 00:05:13,969
But you can do that and you can work backwards.

103
00:05:13,969 --> 00:05:18,354
Listen back to a voice note and you painfully go

104
00:05:18,354 --> 00:05:21,474
through one of those transcribing, where you start and stop

105
00:05:21,474 --> 00:05:23,634
and scrub around it and you fix the errors, but

106
00:05:23,634 --> 00:05:25,875
it's really, really pouring to do that.

107
00:05:26,115 --> 00:05:28,034
So I thought it would be less tedious in the

108
00:05:28,034 --> 00:05:31,714
long term if I just recorded the source of truth.

109
00:05:32,069 --> 00:05:34,389
So it gave me these three minutes snippets.

110
00:05:34,389 --> 00:05:37,509
I recorded them and saved an MP3 and a TXT

111
00:05:37,750 --> 00:05:40,310
in the same folder and I created an error that

112
00:05:40,310 --> 00:05:40,949
data.

113
00:05:41,990 --> 00:05:44,870
So I was very hopeful, quietly, a little bit hopeful

114
00:05:44,870 --> 00:05:47,029
that I would be able, that I could actually fine

115
00:05:47,029 --> 00:05:47,750
tune Whisper.

116
00:05:48,365 --> 00:05:51,085
I want to fine tune Whisper because when I got

117
00:05:51,085 --> 00:05:55,004
into voice tech last November, my wife was in the

118
00:05:55,004 --> 00:05:57,245
US and I was alone at home.

119
00:05:57,324 --> 00:06:01,004
And when crazy people like me do really wild things

120
00:06:01,004 --> 00:06:03,980
like use voice to tech technology.

121
00:06:03,980 --> 00:06:06,939
That was basically when I started doing it, I didn't

122
00:06:06,939 --> 00:06:09,580
feel like a crazy person speaking to myself.

123
00:06:09,980 --> 00:06:12,780
And my expectations weren't that high.

124
00:06:13,180 --> 00:06:17,685
I'd used speech tech now and again, tried it out.

125
00:06:17,685 --> 00:06:18,884
I was like, it'd be really cool if you could

126
00:06:18,884 --> 00:06:22,404
just like speak into your computer and whatever I tried

127
00:06:22,404 --> 00:06:25,925
out that had Linux support was just, it was not

128
00:06:25,925 --> 00:06:26,805
good basically.

129
00:06:27,365 --> 00:06:29,524
And this blew me away from the first go.

130
00:06:29,524 --> 00:06:32,339
I mean, it wasn't one hundred percent accurate out of

131
00:06:32,339 --> 00:06:34,500
the box and it took work, but it was good

132
00:06:34,500 --> 00:06:36,819
enough that there was a solid foundation and it kind

133
00:06:36,819 --> 00:06:41,139
of passed that pivot point that it's actually worth doing

134
00:06:41,139 --> 00:06:41,620
this.

135
00:06:41,939 --> 00:06:43,939
You know, there's a point where it's so like, the

136
00:06:43,939 --> 00:06:46,485
transcript is you don't have to get one hundred percent

137
00:06:46,485 --> 00:06:49,525
accuracy for it to be worth your time for speech

138
00:06:49,525 --> 00:06:51,925
to text to be a worthwhile addition to your productivity.

139
00:06:51,925 --> 00:06:53,685
But you do need to get above, let's say, I

140
00:06:53,685 --> 00:06:55,125
don't know, eighty five percent.

141
00:06:55,605 --> 00:06:58,805
If it's sixty percent or fifty percent, you inevitably say,

142
00:06:59,040 --> 00:07:00,319
Screw it, I'll just type it.

143
00:07:00,319 --> 00:07:03,680
Because you end up missing errors in the transcript and

144
00:07:03,680 --> 00:07:05,040
it becomes actually worse.

145
00:07:05,040 --> 00:07:06,720
You end up in a worse position than you started

146
00:07:06,720 --> 00:07:07,040
with it.

147
00:07:07,040 --> 00:07:08,240
That's been my experience.

148
00:07:08,560 --> 00:07:12,480
So I was like, Oh, this is actually really, really

149
00:07:12,480 --> 00:07:12,960
good now.

150
00:07:12,960 --> 00:07:13,680
How did that happen?

151
00:07:13,680 --> 00:07:17,995
And the answer is ASR, Whisper being open sourced and

152
00:07:18,714 --> 00:07:21,594
the transformer architecture, if you want to go back to

153
00:07:21,594 --> 00:07:26,394
the underpinnings, which really blows my mind and it's on

154
00:07:26,394 --> 00:07:29,830
my list to read through that paper.

155
00:07:30,389 --> 00:07:35,990
All you need is attention as attentively as can be

156
00:07:35,990 --> 00:07:39,350
done with my limited brain because it's super super high

157
00:07:39,350 --> 00:07:43,045
level stuff, super advanced stuff, mean.

158
00:07:43,285 --> 00:07:48,084
That I think of all the things that are fascinating

159
00:07:48,084 --> 00:07:52,564
about the sudden rise in AI and the dramatic capabilities,

160
00:07:53,339 --> 00:07:55,419
I find it fascinating that few people are like, hang

161
00:07:55,419 --> 00:07:58,300
on, you've got this thing that can speak to you

162
00:07:58,300 --> 00:08:00,060
like a chatbot, an LLM.

163
00:08:00,620 --> 00:08:02,860
And then you've got image generation.

164
00:08:02,860 --> 00:08:03,180
Okay.

165
00:08:03,180 --> 00:08:07,100
So firstly, two things on the surface have nothing in

166
00:08:07,100 --> 00:08:07,419
common.

167
00:08:08,365 --> 00:08:12,044
So how did that just happen all at the same

168
00:08:12,044 --> 00:08:12,285
time?

169
00:08:12,285 --> 00:08:15,964
And then when you extend that further, you're like, Suno.

170
00:08:15,964 --> 00:08:19,485
You can sing a song and AI will come up

171
00:08:19,485 --> 00:08:21,165
with an instrumental.

172
00:08:21,485 --> 00:08:23,485
And then you've got Whisper and you're like, Wait a

173
00:08:23,485 --> 00:08:23,725
second.

174
00:08:24,100 --> 00:08:28,180
How did all this stuff If it's all AI, there

175
00:08:28,180 --> 00:08:29,540
has to be some commonality.

176
00:08:29,540 --> 00:08:35,139
Otherwise, are totally different technologies on the surface of it.

177
00:08:35,220 --> 00:08:39,384
And the transformer architecture is, as far as I know,

178
00:08:39,384 --> 00:08:40,264
the answer.

179
00:08:40,264 --> 00:08:42,985
And I can't even say, can't even pretend that I

180
00:08:42,985 --> 00:08:47,384
really understand what the transformer architecture means in-depth.

181
00:08:47,384 --> 00:08:49,865
But I have scanned this and as I said, I

182
00:08:49,865 --> 00:08:52,879
want to print it and really kind of think over

183
00:08:52,879 --> 00:08:54,160
it at some point.

184
00:08:54,879 --> 00:08:58,080
And I'll probably feel bad about myself, I think, because

185
00:08:58,080 --> 00:08:59,679
weren't those guys in twenties?

186
00:09:00,320 --> 00:09:01,840
Like, that's crazy.

187
00:09:02,160 --> 00:09:06,160
I think I asked ChatGPT once who wrote that paper

188
00:09:06,545 --> 00:09:09,264
and how old were they when it was published in

189
00:09:09,264 --> 00:09:09,825
ArcSiv?

190
00:09:09,825 --> 00:09:13,105
And I was expecting like, I don't know, what do

191
00:09:13,105 --> 00:09:13,585
you imagine?

192
00:09:13,585 --> 00:09:15,665
I personally imagine kind of like, you you have these

193
00:09:15,665 --> 00:09:19,745
breakthroughs during COVID and things like that, where like these

194
00:09:19,745 --> 00:09:22,629
kind of really obscure scientists who are in their 50s

195
00:09:22,629 --> 00:09:26,870
and they've just kind of been laboring in labs and

196
00:09:26,870 --> 00:09:29,830
wearily in writing and publishing in kind of obscure academic

197
00:09:29,830 --> 00:09:30,710
publications.

198
00:09:30,870 --> 00:09:33,669
And they finally hit a big or win a Nobel

199
00:09:33,669 --> 00:09:36,235
Prize and then their household names.

200
00:09:36,634 --> 00:09:38,634
So that was kind of what I had in mind.

201
00:09:38,634 --> 00:09:42,154
That was the mental image I'd formed of the birth

202
00:09:42,154 --> 00:09:42,955
of ArcSim.

203
00:09:42,955 --> 00:09:45,595
Like I wasn't expecting twenty somethings in San Francisco.

204
00:09:45,595 --> 00:09:48,794
I thought that was both very funny, very cool, and

205
00:09:48,794 --> 00:09:50,075
actually kind of inspiring.

206
00:09:50,554 --> 00:09:55,230
It's nice to think that people who just you might

207
00:09:55,230 --> 00:09:58,509
put them in the kind of milieu or bubble or

208
00:09:58,509 --> 00:10:02,669
world that you are in incredibly in through a series

209
00:10:02,669 --> 00:10:05,835
of connections that are coming up with such literally world

210
00:10:05,835 --> 00:10:07,835
changing innovations.

211
00:10:07,914 --> 00:10:11,274
So that was I thought anyway, that's that that was

212
00:10:11,274 --> 00:10:11,835
cool.

213
00:10:12,235 --> 00:10:12,554
Okay.

214
00:10:12,554 --> 00:10:13,434
Voice training data.

215
00:10:13,434 --> 00:10:14,154
How are we doing?

216
00:10:14,154 --> 00:10:17,355
We're about ten minutes, and I'm still talking about voice

217
00:10:17,355 --> 00:10:18,235
technology.

218
00:10:18,634 --> 00:10:22,179
So Whisper was brilliant, and I was so excited that

219
00:10:22,179 --> 00:10:25,860
my first instinct was to guess, like, Oh my gosh,

220
00:10:25,860 --> 00:10:28,019
I have to get a really good microphone for this.

221
00:10:28,179 --> 00:10:31,379
So I didn't go on a spending spree because I

222
00:10:31,379 --> 00:10:33,299
said, I'm gonna have to just wait a month and

223
00:10:33,299 --> 00:10:34,740
see if I still use this.

224
00:10:35,220 --> 00:10:38,875
And it just kind of became it's become really part

225
00:10:38,875 --> 00:10:40,955
of my daily routine.

226
00:10:41,754 --> 00:10:44,315
Like if I'm writing an email, I'll record a voice

227
00:10:44,315 --> 00:10:47,595
note and then I've developed and it's nice to see

228
00:10:47,595 --> 00:10:50,759
that everyone is like developing the same things in parallel.

229
00:10:50,759 --> 00:10:53,399
That's kind of a weird thing to say, when I

230
00:10:53,399 --> 00:11:00,279
started working on these prototypes on GitHub, which is where

231
00:11:00,279 --> 00:11:04,039
I just kind of share very freely and loosely ideas

232
00:11:04,039 --> 00:11:06,945
and first iterations on concepts.

233
00:11:09,024 --> 00:11:10,704
And for want of a better word, I called it

234
00:11:10,704 --> 00:11:14,945
like LLM post processing or clean up or basically a

235
00:11:14,945 --> 00:11:17,745
system prompt that after you get back the raw text

236
00:11:17,745 --> 00:11:21,620
from Whisper, you run it through a model and say,

237
00:11:21,620 --> 00:11:26,339
okay, this is crappy text like add sentence structure and,

238
00:11:26,339 --> 00:11:27,459
you know, fix it up.

239
00:11:27,860 --> 00:11:32,579
And now when I'm exploring the different tools that are

240
00:11:32,579 --> 00:11:35,634
out there that people have built, I see quite a

241
00:11:35,634 --> 00:11:39,475
number of projects have basically done the same thing.

242
00:11:40,754 --> 00:11:43,235
Lest that be misconstrued, I'm not saying for a millisecond

243
00:11:43,235 --> 00:11:44,595
that I inspired them.

244
00:11:44,595 --> 00:11:48,034
I'm sure this has been a thing that's been integrated

245
00:11:48,034 --> 00:11:51,290
into tools for a while, but it's the kind of

246
00:11:51,290 --> 00:11:53,690
thing that when you start using these tools every day,

247
00:11:53,690 --> 00:11:57,610
the need for it is almost instantly apparent because text

248
00:11:57,610 --> 00:12:01,529
that doesn't have any punctuation or paragraph spacing takes a

249
00:12:01,529 --> 00:12:03,965
long time to, you know, it takes so long to

250
00:12:03,965 --> 00:12:09,004
get it into a presentable email that again, moves speech

251
00:12:09,004 --> 00:12:13,085
tech into that before that inflection point where you're like,

252
00:12:13,085 --> 00:12:13,965
nah, it's just not worth it.

253
00:12:13,965 --> 00:12:16,924
It's like, it'll just be quicker to type this.

254
00:12:17,279 --> 00:12:19,840
So it's a big, it's a little touch that actually

255
00:12:20,080 --> 00:12:21,200
is a big deal.

256
00:12:21,519 --> 00:12:25,440
So I was on Whisper and I've been using Whisper

257
00:12:25,440 --> 00:12:27,759
and I kind of early on found a couple of

258
00:12:27,759 --> 00:12:28,399
tools.

259
00:12:28,399 --> 00:12:30,639
I couldn't find what I was looking for on Linux,

260
00:12:30,639 --> 00:12:35,924
which is basically just something that'll run-in the background.

261
00:12:35,924 --> 00:12:38,245
You'll give it an API key and it will just

262
00:12:38,245 --> 00:12:43,044
like transcribe with like a little key to start and

263
00:12:43,044 --> 00:12:43,845
stop the dictation.

264
00:12:45,080 --> 00:12:48,440
And the issues where I discovered that like most people

265
00:12:48,440 --> 00:12:52,040
involved in creating these projects were very much focused on

266
00:12:52,040 --> 00:12:55,800
local models, running Whisper locally because you can.

267
00:12:56,279 --> 00:12:58,200
And I tried that a bunch of times and just

268
00:12:58,200 --> 00:13:01,054
never got results that were as good as the cloud.

269
00:13:01,455 --> 00:13:03,615
And when I began looking at the cost of the

270
00:13:03,615 --> 00:13:06,654
speech to text APIs and what I was spending, I

271
00:13:06,654 --> 00:13:09,855
just thought there is it's actually, in my opinion, just

272
00:13:09,855 --> 00:13:13,160
one of the better deals in API spending in the

273
00:13:13,160 --> 00:13:13,480
cloud.

274
00:13:13,480 --> 00:13:15,720
Like, it's just not that expensive for very, very good

275
00:13:15,720 --> 00:13:19,639
models that are much more, you know, you're gonna be

276
00:13:19,639 --> 00:13:22,759
able to run the full model, the latest model versus

277
00:13:22,759 --> 00:13:26,605
whatever you can run on your average GPU unless you

278
00:13:26,605 --> 00:13:28,845
want to buy a crazy GPU.

279
00:13:28,845 --> 00:13:30,044
It doesn't really make sense to me.

280
00:13:30,044 --> 00:13:33,164
Privacy is another concern that I know is kind of

281
00:13:33,164 --> 00:13:35,325
like a very much a separate thing that people just

282
00:13:35,325 --> 00:13:38,845
don't want their voice data and their voice leaving their

283
00:13:38,845 --> 00:13:42,460
local environment maybe for regulatory reasons as well.

284
00:13:42,700 --> 00:13:43,980
But I'm not in that.

285
00:13:44,220 --> 00:13:48,540
I neither really care about people listening to my, grocery

286
00:13:48,540 --> 00:13:51,580
list, consisting of, reminding myself that I need to buy

287
00:13:51,580 --> 00:13:54,779
more beer, Cheetos, and hummus, which is kind of the

288
00:13:55,334 --> 00:13:59,574
three staples of my diet during periods of poor nutrition.

289
00:13:59,894 --> 00:14:02,375
But the kind of stuff that I transcribe, it's just

290
00:14:02,375 --> 00:14:02,694
not.

291
00:14:02,694 --> 00:14:07,814
It's not a privacy thing I'm that sort of sensitive

292
00:14:07,814 --> 00:14:13,269
about and I don't do anything so sensitive or secure

293
00:14:13,269 --> 00:14:14,790
that requires air capping.

294
00:14:15,670 --> 00:14:17,590
I looked at the pricing and especially the kind of

295
00:14:17,590 --> 00:14:18,950
older model mini.

296
00:14:19,590 --> 00:14:21,910
Some of them are very, very affordable and I did

297
00:14:21,910 --> 00:14:26,764
a calculation once with ChatGPT and I was like, okay,

298
00:14:26,764 --> 00:14:30,365
this is the API price for I can't remember whatever

299
00:14:30,365 --> 00:14:31,404
the model was.

300
00:14:31,804 --> 00:14:34,445
Let's say I just go at it like nonstop, which

301
00:14:34,445 --> 00:14:35,565
rarely happens.

302
00:14:35,644 --> 00:14:38,959
Probably, I would say on average I might dictate thirty

303
00:14:38,959 --> 00:14:41,759
to sixty minutes per day if I was probably summing

304
00:14:41,759 --> 00:14:48,000
up the emails, documents, outlines, which is a lot, but

305
00:14:48,000 --> 00:14:50,159
it's it's still a fairly modest amount.

306
00:14:50,159 --> 00:14:51,839
And I was like, well, some days I do go

307
00:14:51,839 --> 00:14:54,934
on like one or two days where I've been usually

308
00:14:54,934 --> 00:14:56,855
when I'm like kind of out of the house and

309
00:14:56,855 --> 00:15:00,535
just have something like I have nothing else to do.

310
00:15:00,535 --> 00:15:03,175
Like if I'm at a hospital, we have a newborn

311
00:15:03,575 --> 00:15:07,299
and you're waiting for like eight hours and hours for

312
00:15:07,299 --> 00:15:08,100
an appointment.

313
00:15:08,179 --> 00:15:12,019
And I would probably have listened to podcasts before becoming

314
00:15:12,019 --> 00:15:12,980
a speech fanatic.

315
00:15:12,980 --> 00:15:15,379
And I'm like, Oh, wait, let me just get down.

316
00:15:15,379 --> 00:15:17,379
Let me just get these ideas out of my head.

317
00:15:17,540 --> 00:15:20,745
And that's when I'll go on my speech binges.

318
00:15:20,745 --> 00:15:22,664
But those are like once every few months, like not

319
00:15:22,664 --> 00:15:23,544
frequently.

320
00:15:23,784 --> 00:15:25,784
But I said, okay, let's just say if I'm going

321
00:15:25,784 --> 00:15:28,184
to price out cloud STT.

322
00:15:28,985 --> 00:15:33,500
If I was like dedicated every second of every waking

323
00:15:33,500 --> 00:15:37,820
hour to transcribing for some odd reason, I mean I'd

324
00:15:37,820 --> 00:15:39,820
have to eat and use the toilet.

325
00:15:40,540 --> 00:15:42,700
There's only so many hours I'm awake for.

326
00:15:42,700 --> 00:15:47,019
So let's just say a maximum of forty five minutes

327
00:15:47,205 --> 00:15:49,205
in the hour, then I said, All right, let's just

328
00:15:49,205 --> 00:15:50,165
say fifty.

329
00:15:50,644 --> 00:15:51,365
Who knows?

330
00:15:51,365 --> 00:15:52,804
You're dictating on the toilet.

331
00:15:52,804 --> 00:15:53,605
We do it.

332
00:15:53,924 --> 00:15:56,884
So you could just do sixty, but whatever I did

333
00:15:57,125 --> 00:16:01,179
and every day, like you're going flat out seven days

334
00:16:01,179 --> 00:16:02,620
a week dictating nonstop.

335
00:16:02,620 --> 00:16:05,579
I was like, What's my monthly API bill going to

336
00:16:05,579 --> 00:16:06,700
be at this price?

337
00:16:06,779 --> 00:16:09,339
And it came out to like seventy or eighty bucks.

338
00:16:09,339 --> 00:16:12,620
And I was like, Well, that would be an extraordinary

339
00:16:12,940 --> 00:16:14,379
amount of dictation.

340
00:16:14,379 --> 00:16:18,105
And I would hope that there was some compelling reason

341
00:16:18,745 --> 00:16:21,784
worth more than seventy dollars that I embarked upon that

342
00:16:21,784 --> 00:16:22,424
project.

343
00:16:22,664 --> 00:16:24,585
So given that that's kind of the max point for

344
00:16:24,585 --> 00:16:27,304
me I said that's actually very very affordable.

345
00:16:28,024 --> 00:16:30,504
Now you're gonna if you want to spec out the

346
00:16:30,504 --> 00:16:33,909
costs and you want to do the post processing that

347
00:16:33,909 --> 00:16:36,789
I really do feel is valuable, that's going to cost

348
00:16:36,789 --> 00:16:37,750
some more as well.

349
00:16:38,070 --> 00:16:43,269
Unless you're using Gemini, which needless to say is a

350
00:16:43,269 --> 00:16:45,190
random person sitting in Jerusalem.

351
00:16:45,855 --> 00:16:49,455
I have no affiliation nor with Google nor Anthropic nor

352
00:16:49,455 --> 00:16:52,414
Gemini nor any major tech vendor for that matter.

353
00:16:53,855 --> 00:16:57,215
I like Gemini not so much as a everyday model.

354
00:16:57,455 --> 00:16:59,934
It's kind of underwhelmed in that respect, I would say.

355
00:17:00,379 --> 00:17:02,779
But for multimodal, I think it's got a lot to

356
00:17:02,779 --> 00:17:03,339
offer.

357
00:17:03,659 --> 00:17:07,179
And I think that the transcribing functionality whereby it can,

358
00:17:08,059 --> 00:17:12,380
process audio with a system prompt and both give you

359
00:17:12,380 --> 00:17:13,900
transcription that's cleaned up.

360
00:17:13,900 --> 00:17:15,339
That reduces two steps to one.

361
00:17:15,835 --> 00:17:18,954
And that for me is a very, very big deal.

362
00:17:18,955 --> 00:17:22,474
And I feel like even Google hasn't really sort of

363
00:17:22,555 --> 00:17:27,195
thought through how useful the that modality is and what

364
00:17:27,195 --> 00:17:29,700
kind of use cases you can achieve with it.

365
00:17:29,700 --> 00:17:32,339
Because I found in the course of this year just

366
00:17:32,339 --> 00:17:38,019
an endless list of really kind of system prompt stuff

367
00:17:38,019 --> 00:17:40,900
that I can say, okay, I've used it to capture

368
00:17:40,900 --> 00:17:44,115
context data for AI, which is literally I might speak

369
00:17:44,115 --> 00:17:46,755
for if I wanted to have a good bank of

370
00:17:46,755 --> 00:17:50,035
context data about who knows my childhood.

371
00:17:50,434 --> 00:17:54,355
More realistically, maybe my career goals, something that would just

372
00:17:54,355 --> 00:17:56,195
be like really boring to type out.

373
00:17:56,195 --> 00:18:00,500
So I'll just like sit in my car and record

374
00:18:00,500 --> 00:18:01,460
it for ten minutes.

375
00:18:01,460 --> 00:18:03,779
And that ten minutes you get a lot of information

376
00:18:03,779 --> 00:18:04,419
in.

377
00:18:05,619 --> 00:18:07,700
Emails, which is short text.

378
00:18:08,660 --> 00:18:10,419
Just there is a whole bunch.

379
00:18:10,420 --> 00:18:13,375
And all these workflows kind of require a little bit

380
00:18:13,375 --> 00:18:15,134
of treatment afterwards and different treatment.

381
00:18:15,134 --> 00:18:18,414
My context pipeline is kind of like just extract the

382
00:18:18,414 --> 00:18:19,295
bare essentials.

383
00:18:19,295 --> 00:18:22,174
You end up with me talking very loosely about sort

384
00:18:22,174 --> 00:18:24,494
of what I've done in my career, where I've worked,

385
00:18:24,494 --> 00:18:25,454
where I might like to work.

386
00:18:26,000 --> 00:18:29,119
And it goes, it condenses that down to very robotic

387
00:18:29,119 --> 00:18:32,720
language that is easy to chunk parse and maybe put

388
00:18:32,720 --> 00:18:34,000
into a vector database.

389
00:18:34,000 --> 00:18:36,240
Daniel has worked in technology.

390
00:18:36,240 --> 00:18:39,840
Daniel has been working in, know, stuff like that.

391
00:18:39,840 --> 00:18:43,055
That's not how you would speak, but I figure it's

392
00:18:43,055 --> 00:18:46,494
probably easier to parse for, after all, robots.

393
00:18:46,815 --> 00:18:48,734
So we've almost got to twenty minutes and this is

394
00:18:48,734 --> 00:18:53,134
actually a success because I wasted twenty minutes of my

395
00:18:53,535 --> 00:18:57,200
of the evening speaking into you in microphone and the

396
00:18:57,200 --> 00:19:01,119
levels were shot and was clipping and I said I

397
00:19:01,119 --> 00:19:02,400
can't really do an evaluation.

398
00:19:02,400 --> 00:19:03,440
I have to be fair.

399
00:19:03,440 --> 00:19:06,400
I have to give the models a chance to do

400
00:19:06,400 --> 00:19:06,960
their thing.

401
00:19:07,505 --> 00:19:09,585
What am I hoping to achieve in this?

402
00:19:09,585 --> 00:19:11,664
Okay, my fine tune was a dud as mentioned.

403
00:19:11,745 --> 00:19:15,265
Deepgram STT, I'm really, really hopeful that this prototype will

404
00:19:15,265 --> 00:19:18,065
work and it's a build in public open source so

405
00:19:18,065 --> 00:19:20,384
anyone is welcome to use it if I make anything

406
00:19:20,384 --> 00:19:20,705
good.

407
00:19:21,640 --> 00:19:23,880
But that was really exciting for me last night when

408
00:19:23,880 --> 00:19:28,920
after hours of trying my own prototype, seeing someone just

409
00:19:28,920 --> 00:19:32,119
made something that works like that, you you're not gonna

410
00:19:32,119 --> 00:19:36,454
have to build a custom conda environment and image.

411
00:19:36,454 --> 00:19:40,054
I have AMD GPU which makes things much more complicated.

412
00:19:40,294 --> 00:19:42,694
I didn't find it and I was about to give

413
00:19:42,694 --> 00:19:43,974
up and I said, All right, let me just give

414
00:19:43,974 --> 00:19:46,535
Deepgram's Linux thing a shot.

415
00:19:47,109 --> 00:19:49,669
And if this doesn't work, I'm just gonna go back

416
00:19:49,669 --> 00:19:51,429
to trying to vibe code something myself.

417
00:19:51,750 --> 00:19:55,589
And when I ran the script, I was using Cloud

418
00:19:55,589 --> 00:19:59,109
Code to do the installation process, it ran the script

419
00:19:59,109 --> 00:20:01,269
and, oh my gosh, it works just like that.

420
00:20:01,904 --> 00:20:06,065
The tricky thing for all those who wants to know

421
00:20:06,065 --> 00:20:11,505
all the nitty, ditty, nitty gritty details was that I

422
00:20:11,505 --> 00:20:14,704
don't think it was actually struggling with transcription, but pasting

423
00:20:14,785 --> 00:20:17,619
Weyland makes life very hard.

424
00:20:17,619 --> 00:20:19,220
And I think there was something not running at the

425
00:20:19,220 --> 00:20:19,779
right time.

426
00:20:19,779 --> 00:20:23,059
Anyway, Deepgram, I looked at how they actually handle that

427
00:20:23,059 --> 00:20:25,220
because it worked out of the box when other stuff

428
00:20:25,220 --> 00:20:25,859
didn't.

429
00:20:26,180 --> 00:20:28,980
And it was quite a clever little mechanism.

430
00:20:29,575 --> 00:20:32,215
And but more so than that, the accuracy was brilliant.

431
00:20:32,215 --> 00:20:33,654
Now what am I what am I doing here?

432
00:20:33,654 --> 00:20:37,255
This is gonna be a twenty minute audio sample.

433
00:20:38,455 --> 00:20:42,490
And I'm I think I've done one or two of

434
00:20:42,490 --> 00:20:47,210
these before, but I did it with short, snappy voice

435
00:20:47,210 --> 00:20:47,690
notes.

436
00:20:47,690 --> 00:20:49,450
This is kind of long form.

437
00:20:49,529 --> 00:20:52,009
This actually might be a better approximation for what's useful

438
00:20:52,009 --> 00:20:53,929
to me than voice memos.

439
00:20:53,929 --> 00:20:56,974
Like, I need to buy three liters of milk tomorrow

440
00:20:56,974 --> 00:21:00,255
and peter bread, which is probably how half my voice

441
00:21:00,255 --> 00:21:00,815
notes sound.

442
00:21:00,815 --> 00:21:04,174
Like if anyone were to find my phone they'd be

443
00:21:04,174 --> 00:21:06,014
like this is the most boring person in the world.

444
00:21:06,095 --> 00:21:10,130
Although actually there are some journaling thoughts as well, but

445
00:21:10,130 --> 00:21:11,890
it's a lot of content like that.

446
00:21:11,890 --> 00:21:14,690
And the probably for the evaluation, the most useful thing

447
00:21:14,690 --> 00:21:21,914
is slightly obscure tech, GitHub, Nucleano, hugging face, not so

448
00:21:21,914 --> 00:21:24,554
obscure that it's not gonna have a chance of knowing

449
00:21:24,554 --> 00:21:27,274
it, but hopefully sufficiently well known that the model should

450
00:21:27,274 --> 00:21:27,914
get it.

451
00:21:27,994 --> 00:21:30,075
I tried to do a little bit of speaking really

452
00:21:30,075 --> 00:21:32,474
fast and speaking very slowly.

453
00:21:32,474 --> 00:21:35,609
Would say in general, I've spoken, delivered this at a

454
00:21:35,609 --> 00:21:39,210
faster pace than I usually would owing to strong coffee

455
00:21:39,210 --> 00:21:40,650
flowing through my bloodstream.

456
00:21:41,210 --> 00:21:43,609
And the thing that I'm not gonna get in this

457
00:21:43,609 --> 00:21:46,170
benchmark is background noise, which in my first take that

458
00:21:46,170 --> 00:21:48,535
I had to get rid of, my wife came in

459
00:21:48,535 --> 00:21:51,575
with my son and for a good night kiss.

460
00:21:51,654 --> 00:21:55,174
And that actually would have been super helpful to get

461
00:21:55,174 --> 00:21:57,894
in because it was non diarized or if we had

462
00:21:57,894 --> 00:21:58,775
diarization.

463
00:21:59,414 --> 00:22:01,494
A female, I could say, I want the male voice

464
00:22:01,494 --> 00:22:03,174
and that wasn't intended for transcription.

465
00:22:04,589 --> 00:22:06,349
And we're not going to get background noise like people

466
00:22:06,349 --> 00:22:09,069
honking their horns, which is something I've done in my

467
00:22:09,230 --> 00:22:11,950
main data set where I am trying to go back

468
00:22:11,950 --> 00:22:15,150
to some of my voice notes, annotate them and run

469
00:22:15,150 --> 00:22:15,789
a benchmark.

470
00:22:15,789 --> 00:22:18,345
But this is going to be just a pure quick

471
00:22:18,345 --> 00:22:19,144
test.

472
00:22:19,865 --> 00:22:24,105
And as someone I'm working on a voice note idea.

473
00:22:24,105 --> 00:22:28,265
That's my sort of end motivation besides thinking it's an

474
00:22:28,265 --> 00:22:31,865
absolutely outstanding technology that's coming to viability.

475
00:22:31,865 --> 00:22:34,480
And really, I know this sounds cheesy, can actually have

476
00:22:34,480 --> 00:22:36,559
a very transformative effect.

477
00:22:38,000 --> 00:22:43,200
Voice technology has been life changing for folks living with

478
00:22:44,079 --> 00:22:45,119
disabilities.

479
00:22:46,000 --> 00:22:48,625
And I think there's something really nice about the fact

480
00:22:48,625 --> 00:22:52,625
that it can also benefit folks who are able-bodied and

481
00:22:52,625 --> 00:22:57,984
we can all in different ways make this tech as

482
00:22:57,984 --> 00:23:00,785
useful as possible regardless of the exact way that we're

483
00:23:00,785 --> 00:23:01,105
using it.

484
00:23:02,279 --> 00:23:04,519
And I think there's something very powerful in that, and

485
00:23:04,519 --> 00:23:05,639
it can be very cool.

486
00:23:06,200 --> 00:23:07,639
I see huge potential.

487
00:23:07,639 --> 00:23:09,399
What excites me about voice tech?

488
00:23:09,799 --> 00:23:11,239
A lot of things actually.

489
00:23:12,200 --> 00:23:14,919
Firstly, the fact that it's cheap and accurate, as I

490
00:23:14,919 --> 00:23:17,865
mentioned at the very start of this, and it's getting

491
00:23:17,865 --> 00:23:20,184
better and better with stuff like accent handling.

492
00:23:20,825 --> 00:23:23,384
I'm not sure my fine tune will actually ever come

493
00:23:23,384 --> 00:23:25,305
to fruition in the sense that I'll use it day

494
00:23:25,305 --> 00:23:26,664
to day as I imagine.

495
00:23:26,744 --> 00:23:30,585
I get like superb, flawless words error rates because I'm

496
00:23:30,585 --> 00:23:35,029
just kind of skeptical about local speech to text, as

497
00:23:35,029 --> 00:23:35,750
I mentioned.

498
00:23:36,150 --> 00:23:39,910
And I think the pace of innovation and improvement in

499
00:23:39,910 --> 00:23:42,390
the models, the main reasons for fine tuning from what

500
00:23:42,390 --> 00:23:46,230
I've seen have been people who are something that really

501
00:23:46,230 --> 00:23:50,455
blows blows my mind about ASR is the idea that

502
00:23:50,455 --> 00:23:55,654
it's inherently ailingual or multilingual, phonetic based.

503
00:23:56,375 --> 00:24:00,455
So as folks who use speak very obscure languages that

504
00:24:00,455 --> 00:24:03,174
there may be very there might be a paucity of

505
00:24:02,309 --> 00:24:05,110
training data or almost none at all, and therefore the

506
00:24:05,110 --> 00:24:06,870
accuracy is significantly reduced.

507
00:24:06,870 --> 00:24:11,430
Or folks in very critical environments, I know there are

508
00:24:11,590 --> 00:24:15,430
this is used extensively in medical transcription and dispatcher work

509
00:24:15,430 --> 00:24:19,144
as, you know the call centers who send out ambulances

510
00:24:19,144 --> 00:24:19,944
etc.

511
00:24:20,345 --> 00:24:23,625
Where accuracy is absolutely paramount and in the case of

512
00:24:23,625 --> 00:24:27,625
doctors radiologists they might be using very specialized vocab all

513
00:24:27,625 --> 00:24:27,945
the time.

514
00:24:28,710 --> 00:24:30,309
So those are kind of the main two things, and

515
00:24:30,309 --> 00:24:32,230
I'm not sure that really just for trying to make

516
00:24:32,230 --> 00:24:36,470
it better on a few random tech words with my

517
00:24:36,470 --> 00:24:39,509
slightly I mean, I have an accent, but, like, not,

518
00:24:39,509 --> 00:24:42,549
you know, an accent that a few other million people

519
00:24:42,950 --> 00:24:43,990
have ish.

520
00:24:44,765 --> 00:24:48,045
I'm not sure that my little fine tune is gonna

521
00:24:48,045 --> 00:24:52,684
actually like, the bump in word error reduction, if I

522
00:24:52,684 --> 00:24:54,285
ever actually figure out how to do it and get

523
00:24:54,285 --> 00:24:56,445
it up to the cloud, by the time we've done

524
00:24:56,445 --> 00:25:00,039
that, I suspect that the next generation of ASR will

525
00:25:00,039 --> 00:25:01,799
just be so good that it will kind of be,

526
00:25:02,039 --> 00:25:04,039
well, that would have been cool if it worked out,

527
00:25:04,039 --> 00:25:05,559
but I'll just use this instead.

528
00:25:05,799 --> 00:25:10,759
So that's gonna be it for today's episode of voice

529
00:25:10,759 --> 00:25:11,720
training data.

530
00:25:11,960 --> 00:25:14,335
Single, long shot evaluation.

531
00:25:14,575 --> 00:25:15,774
Who am I gonna compare?

532
00:25:16,494 --> 00:25:18,654
Whisper is always good as a benchmark, but I'm more

533
00:25:18,654 --> 00:25:22,255
interested in seeing Whisper head to head with two things

534
00:25:22,255 --> 00:25:22,974
really.

535
00:25:23,375 --> 00:25:25,214
One is Whisper variants.

536
00:25:25,214 --> 00:25:27,775
So you've got these projects like Faster Whisper.

537
00:25:29,190 --> 00:25:30,069
Distill Whisper.

538
00:25:30,069 --> 00:25:30,789
It's a bit confusing.

539
00:25:30,789 --> 00:25:31,989
There's a whole bunch of them.

540
00:25:32,230 --> 00:25:35,190
And the emerging ASRs, which are also a thing.

541
00:25:35,349 --> 00:25:37,190
My intention for this is I'm not sure I'm gonna

542
00:25:37,190 --> 00:25:39,990
have the time in any point in the foreseeable future

543
00:25:39,990 --> 00:25:44,855
to go back to this whole episode and create a

544
00:25:44,855 --> 00:25:48,374
proper source truth where I fix everything.

545
00:25:49,335 --> 00:25:51,974
Might do it if I can get one transcription that's

546
00:25:51,974 --> 00:25:54,214
sufficiently close to perfection.

547
00:25:55,014 --> 00:25:58,480
But what I would actually love to do on Hugging

548
00:25:58,480 --> 00:26:00,559
Face, I think would be a great probably how I

549
00:26:00,559 --> 00:26:04,480
might visualize this is having the audio waveform play and

550
00:26:04,480 --> 00:26:08,960
then have the transcript for each model below it and

551
00:26:08,960 --> 00:26:13,845
maybe even a, like, you know, to scale and maybe

552
00:26:13,845 --> 00:26:16,724
even a local one as well, like local whisper versus

553
00:26:16,724 --> 00:26:19,764
OpenAI API, etcetera.

554
00:26:19,845 --> 00:26:23,204
And I can then actually listen back to segments or

555
00:26:23,204 --> 00:26:25,365
anyone who wants to can listen back to segments of

556
00:26:25,365 --> 00:26:30,299
this recording and see where a particular model struggled and

557
00:26:30,299 --> 00:26:33,179
others didn't as well as the sort of headline finding

558
00:26:33,179 --> 00:26:35,659
of which had the best W E R but that

559
00:26:35,659 --> 00:26:37,739
would require the source of truth.

560
00:26:37,740 --> 00:26:38,539
Okay, that's it.

561
00:26:38,505 --> 00:26:41,065
I hope this was, I don't know, maybe useful for

562
00:26:41,065 --> 00:26:42,984
other folks interested in STT.

563
00:26:43,065 --> 00:26:46,025
You want to see I always think I've just said

564
00:26:46,025 --> 00:26:47,704
it as something I didn't intend to.

565
00:26:47,944 --> 00:26:49,704
STT, I said for those.

566
00:26:49,704 --> 00:26:53,129
Listen carefully, including hopefully the models themselves.

567
00:26:53,369 --> 00:26:55,129
This has been myself, Daniel Rosol.

568
00:26:55,129 --> 00:26:59,450
For more jumbled repositories about my roving interest in AI

569
00:26:59,450 --> 00:27:04,089
but particularly AgenTic, MCP and VoiceTech you can find me

570
00:27:04,089 --> 00:27:05,769
on GitHub.

571
00:27:06,009 --> 00:27:06,730
Hugging Face.

572
00:27:08,125 --> 00:27:09,004
Where else?

573
00:27:09,005 --> 00:27:11,805
DanielRosel dot com, which is my personal website, as well

574
00:27:11,805 --> 00:27:15,565
as this podcast whose name I sadly cannot remember.

575
00:27:15,724 --> 00:27:16,765
Until next time.

576
00:27:16,765 --> 00:27:17,404
Thanks for listening.

